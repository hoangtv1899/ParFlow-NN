{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick start guide to CBRAIN-CAM\n",
    "\n",
    "Hello to all you poor souls that have to use this repository. I apologize for all the sloppy coding and deprecated options. I promise that cbrain_v2 is coming!\n",
    "\n",
    "For now this is an overview of the essential workflow. Feel free to ask me questions via email!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SPCAM with the appropriate output\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The first step is to preprocess the data from the SPCAM NetCDF outputs to a format that is quickly readable for the neural network.\n",
    "\n",
    "The preprocessing functions are `preprocess_aqua.py` and `shuffle_ds.py` in the `cbrain` subdirectory.\n",
    "\n",
    "The raw SPCAM output will look something like this\n",
    "\n",
    "```\n",
    "AndKua_aqua_SPCAM3.0_sp_fbp_f4.cam2.h1.0000-01-01-00000.nc\n",
    "AndKua_aqua_SPCAM3.0_sp_fbp_f4.cam2.h1.0000-01-02-00000.nc\n",
    "AndKua_aqua_SPCAM3.0_sp_fbp_f4.cam2.h1.0000-01-03-00000.nc\n",
    "...\n",
    "```\n",
    "\n",
    "`preprocess_aqua.py` is meant to be called from the command line. The arguments can either be passed in directly or in a config.yml file or both, in which case the order of importance is command line > config > default.\n",
    "\n",
    "The most important arguments are:\n",
    "- inputs: List of input variables, e.g. [TBP, QBP, VBP, PS, SOLIN, SHFLX, LHFLX]. `_BP` variables are computed as: `TAP - TPHYSTND*dt`\n",
    "- outputs: List of output variables, e.g. [TPHYSTND, PHQ, FSNT, FSNS, FLNT, FLNS, PRECT]\n",
    "- in_dir: Directory where SPCAM files are stored. I strongly recommend putting the files on a fast drive (e.g. /scratch/ on Greenplanet)\n",
    "- aqua_names: Here you can put a string with placeholders `*`, e.g. `'*.h1.0001-*-*-*'` for all files from year 1 or `'*.h1.0000-*-1[7-9]-*'` for day 17-19 from each month of year 0.\n",
    "- out_dir: directory where the processed files will be stored\n",
    "- out_pref: Prefix for the output files. Give them some descriptive name, e.g. `my_experiment1`\n",
    "- ext_norm: This is depricated and ugly. If ext_norm = None (default), a normalization file will be computed from the data. This normalization file contains means, standard deviations, etc. For a full year of data this can take a long time. For this reason I chose to compute the normalization files for a sample of the data. The differences are small. If ext_norm is some string (just pick any jiberish) no normalization file will be computed but the data will still not be normalized. This is controlled by the `norm_features` and `norm_targets` options which are None and should stay so. The normalization happens later on the fly during network training.\n",
    "\n",
    "There are also some arguments hard-coded, for example the normalization factors that are saved in the normalization file. They are at the top of `preprocess_aqua.py` in a dictionary.\n",
    "\n",
    "An example usage would be:\n",
    "```commandline\n",
    "python preprocess_aqua.py --config_file ../pp_config/fbp_engy_ess.yml --in_dir /beegfs/DATA/pritchard/srasp/fluxbypass_aqua/ --aqua_names '*.h1.0001-*-0[5-9]-*' --out_dir /beegfs/DATA/pritchard/srasp/preprocessed_data/ --out_pref fbp_engy_ess_sample_train\n",
    "```\n",
    "which will produce `out_pref` + features.nc, + targets.nc and + norm.nc files in `out_dir`.\n",
    "\n",
    "A validation set has to be created separately by simply chosing a different time interval with `aqua_names`\n",
    "\n",
    "\n",
    "`shuffle_ds.py` pre-randomizes the data which is important for network training (not required for validation set). Its use is simple:\n",
    "```commandline\n",
    "python $REPO/cbrain/shuffle_ds.py --pref <out_dir>/<out_pref>\n",
    "```\n",
    "It will create a file with the appendix `_shuffle`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network training\n",
    "\n",
    "You can either train a neural network using the script run_experiment or do it yourself in a notebook like this. Let's go through the basic steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required cbrain functions\n",
    "from cbrain.imports import *\n",
    "from cbrain.data_generator import *\n",
    "from cbrain.models import *\n",
    "from cbrain.utils import limit_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running on the GPU, execute this\n",
    "# Otherwise tensorflow will use ALL your GPU RAM for no reason\n",
    "limit_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define the data generators for the training and validation data.\n",
    "\n",
    "The DataGenerator class takes the following important arguments:\n",
    "- data_dir: path where preprocessed files are stored. Again, I recommend using an SSD for this.\n",
    "- feature_fn: name of feature file, e.g. `bp_engy_ess_sample_train_shuffle_features.nc'\n",
    "- target_fn: corresponding targets file\n",
    "- batch size: the batch size\n",
    "- norm_fn: this is the normalization file to be used. Has to be the same for validation and training set.\n",
    "- fsub, fdiv, tsub, tmult: These indicate how the features and targets are normalized. The names correspond to the normalization file, but some names also have special instructions inside the DataGenerator. My default values are `fsub: feature_means, fdiv: max_rs, tmult: target_conv, tsub: None`. For more info dig the code or ask me :)\n",
    "- shuffle: Whether the batches are shuffled pseudo-randomly. Should the True for training, doesn't matter for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '/local/S.Rasp/preprocessed_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator will have 23494656 samples in 45888 batches\n",
      "Features have shape 94; targets have shape 65\n"
     ]
    }
   ],
   "source": [
    "train_gen = DataGenerator(\n",
    "    data_dir=DATADIR, \n",
    "    feature_fn='fbp_engy_ess_train_sample1_shuffle_features.nc',\n",
    "    target_fn='fbp_engy_ess_train_sample1_shuffle_targets.nc',\n",
    "    batch_size=512,\n",
    "    norm_fn='fbp_engy_ess_train_sample1_norm.nc',\n",
    "    fsub='feature_means', \n",
    "    fdiv='feature_stds', \n",
    "    tmult='target_conv',\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator will have 23494656 samples in 45888 batches\n",
      "Features have shape 94; targets have shape 65\n"
     ]
    }
   ],
   "source": [
    "# Same for validation\n",
    "valid_gen = DataGenerator(\n",
    "    data_dir=DATADIR, \n",
    "    feature_fn='fbp_engy_ess_valid_sample1_features.nc',\n",
    "    target_fn='fbp_engy_ess_valid_sample1_targets.nc',\n",
    "    batch_size=512,\n",
    "    norm_fn='fbp_engy_ess_train_sample1_norm.nc',  # SAME NORMALIZATION FILE!\n",
    "    fsub='feature_means', \n",
    "    fdiv='feature_stds', \n",
    "    tmult='target_conv',\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to build a neural network. For this we can use the functions in cbrain.models or write our own network. Let's do this real quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(512, input_shape=(train_gen.feature_shape,), activation='relu'),  # Input layer of size 94\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(train_gen.target_shape, activation='linear')  # output layer of size 65\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               48640     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 65)                33345     \n",
      "=================================================================\n",
      "Total params: 344,641\n",
      "Trainable params: 344,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "45888/45888 [==============================] - 373s 8ms/step - loss: 0.0059 - val_loss: 0.0057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fac822a3908>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally train the model\n",
    "model.fit_generator(\n",
    "    train_gen.return_generator(),   # This actually returns the generator\n",
    "    train_gen.n_batches,\n",
    "    epochs=1,\n",
    "    validation_data=valid_gen.return_generator(),\n",
    "    validation_steps=valid_gen.n_batches,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can also see how long it takes on my GPU in Munich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run_experiment.py script basically does what we just did but has a lot more options. Most of them are not active by default. The final experiment for the PNAS paper is D025 which has the following config file\n",
    "```\n",
    "exp_name: D025_fbp_engy_ess_ref_fullyear_ref_longtrain\n",
    "data_dir: /scratch/srasp/preprocessed_data/\n",
    "train_fn: fbp_engy_ess_ref_train_fullyear_shuffle\n",
    "valid_fn: fbp_engy_ess_valid_sample1\n",
    "norm_fn: fbp_engy_ess_ref_train_sample1_norm.nc\n",
    "fsub: feature_means\n",
    "fdiv: max_rs\n",
    "tmult: target_conv\n",
    "activation: LeakyReLU\n",
    "hidden_layers: [256,256,256,256,256,256,256,256,256]\n",
    "loss: mse\n",
    "log_dir: ./logs/\n",
    "epochs: 18\n",
    "lr_step: 3\n",
    "valid_after: True\n",
    "```\n",
    "\n",
    "Some explanation:\n",
    "- log_dir: This simply saves a Tensorboard log. I would not worry about this for now since it doesn't really give more information that the scores.\n",
    "- lr_step: In addition to the simple example above, I aplemented a learning rate scheduler. I basically copied this: https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/ lr_step defines after how many epochs the learning rate is dropped.\n",
    "- valid_after: If this is true, the validation score is only computed for the last epoch. This is reasonable once you know that your model behaves ok. Computing the validation score actually takes some time.\n",
    "- model_dir and exp_name: The keras model is saved after training in the directory specified with the experiment name. We need the saved model to implement into CAM later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get the model into CAM\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
